{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow numpy requests bs4\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: scraped_data\\story_0.txt\n",
      "Saved: scraped_data\\story_1.txt\n",
      "Saved: scraped_data\\story_2.txt\n",
      "Saved: scraped_data\\story_3.txt\n",
      "Saved: scraped_data\\story_4.txt\n",
      "Saved: scraped_data\\story_5.txt\n",
      "Error scraping https://www.fairytales.biz/: 406 Client Error: Not Acceptable for url: https://www.fairytales.biz/\n",
      "Saved: scraped_data\\story_7.txt\n",
      "Saved: scraped_data\\story_8.txt\n",
      "Saved: scraped_data\\story_9.txt\n",
      "Scraping completed. Stories saved in 'scraped_stories' directory.\n"
     ]
    }
   ],
   "source": [
    "# Directory to store scraped data\n",
    "data_dir = \"scraped_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# List of URLs to scrape (kids story related articles)\n",
    "story_urls = [\n",
    "    \"https://www.storyberries.com/\",\n",
    "    \"http://www.magickeys.com/books/\",\n",
    "    \"https://www.mainlesson.com/\",\n",
    "    \"https://www.storynory.com/\",\n",
    "    \"https://www.worldoftales.com/\",\n",
    "    \"https://www.freekidsbooks.org/\",\n",
    "    \"https://www.fairytales.biz/\",\n",
    "    \"https://americanliterature.com/childrens-stories\",\n",
    "    \"https://www.kidsworldfun.com/story-contest/\",\n",
    "    \"https://www.shortkidstories.com/\"\n",
    "]\n",
    "\n",
    "# Function to extract text content from a webpage\n",
    "def scrape_story(url, file_index):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extracting text content - Adjust selectors based on website structure\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        story_text = \"\\n\".join([para.get_text() for para in paragraphs if len(para.get_text()) > 30])\n",
    "\n",
    "        # Save story to a text file\n",
    "        if story_text:\n",
    "            file_path = os.path.join(data_dir, f\"story_{file_index}.txt\")\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(story_text)\n",
    "            print(f\"Saved: {file_path}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Iterative scraping of story websites\n",
    "for i, url in enumerate(story_urls):\n",
    "    scrape_story(url, i)\n",
    "    time.sleep(2)  # To avoid getting blocked\n",
    "\n",
    "print(\"Scraping completed. Stories saved in 'scraped_stories' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in text: 33523\n"
     ]
    }
   ],
   "source": [
    "# Read all files and combine text\n",
    "def load_data(directory):\n",
    "    text_data = \"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data += file.read() + \"\\n\"\n",
    "    return text_data\n",
    "\n",
    "# Load and preprocess text\n",
    "text = load_data(data_dir)\n",
    "print(f\"Total characters in text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5054, 179), (5054, 1894))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Generate input sequences\n",
    "input_sequences = []\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])\n",
    "\n",
    "# Padding sequences\n",
    "max_seq_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# Split into X (features) and y (labels)\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 153ms/step - accuracy: 0.0220 - loss: 7.2044\n",
      "Epoch 2/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 102ms/step - accuracy: 0.0305 - loss: 6.6705\n",
      "Epoch 3/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 120ms/step - accuracy: 0.0322 - loss: 6.4381\n",
      "Epoch 4/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 283ms/step - accuracy: 0.0344 - loss: 6.2356\n",
      "Epoch 5/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 237ms/step - accuracy: 0.0414 - loss: 6.0248\n",
      "Epoch 6/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - accuracy: 0.0421 - loss: 5.8614\n",
      "Epoch 7/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - accuracy: 0.0480 - loss: 5.6693\n",
      "Epoch 8/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 114ms/step - accuracy: 0.0519 - loss: 5.4465\n",
      "Epoch 9/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 110ms/step - accuracy: 0.0443 - loss: 5.3169\n",
      "Epoch 10/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - accuracy: 0.0603 - loss: 5.1246\n",
      "Epoch 11/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 315ms/step - accuracy: 0.0817 - loss: 4.9241\n",
      "Epoch 12/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 187ms/step - accuracy: 0.0867 - loss: 4.7714\n",
      "Epoch 13/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 137ms/step - accuracy: 0.0959 - loss: 4.6271\n",
      "Epoch 14/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 113ms/step - accuracy: 0.1112 - loss: 4.4572\n",
      "Epoch 15/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 107ms/step - accuracy: 0.1198 - loss: 4.3093\n",
      "Epoch 16/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - accuracy: 0.1318 - loss: 4.1635\n",
      "Epoch 17/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - accuracy: 0.1576 - loss: 3.9968\n",
      "Epoch 18/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - accuracy: 0.1757 - loss: 3.8064\n",
      "Epoch 19/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 106ms/step - accuracy: 0.1971 - loss: 3.6464\n",
      "Epoch 20/100\n",
      "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 171ms/step - accuracy: 0.2207 - loss: 3.4720\n",
      "Epoch 21/100\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(total_words, 50, input_length=max_seq_length-1),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation function\n",
    "def generate_text(seed_text, next_words=10):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                seed_text += \" \" + word\n",
    "                break\n",
    "    return seed_text\n",
    "\n",
    "# Generate text from a seed phrase\n",
    "print(generate_text(\"A boy named Ivan   \", next_words=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
